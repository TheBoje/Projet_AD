{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initialisation des variables\n",
    "\n",
    "$$f(x)=\\frac{1}{4\\pi}\\exp \\left \\{ -\\frac{1}{2} \\left (x-\\begin{pmatrix}2\\\\ 2\\end{pmatrix} \\right)^T\\begin{bmatrix}2 & 0\\\\ 0 & 2\\end{bmatrix}^{-1}\\left (x-\\begin{pmatrix}2\\\\ 2\\end{pmatrix} \\right ) \\right \\}$$\n",
    "$$f(x)=\\frac{1}{12\\pi}\\exp\\left \\{ -\\frac{1}{2}\\left(x-\\begin{pmatrix}-4\\\\ -4\\end{pmatrix} \\right)^T\\begin{bmatrix}6 & 0\\\\ 0 & 6\\end{bmatrix}^{-1}\\left (x-\\begin{pmatrix}-4\\\\ -4\\end{pmatrix}\\right ) \\right \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Simulation de n = 256 données X=(X1, X2) suivant une loi normale bidimensionnelle\n",
    "# de moyenne (0,0), de la variance Var(X1) = 1, Var(X2) = 1 et la covariance Cov(X1, X2) = 0.5\n",
    "\n",
    "\n",
    "mean_1 = np.array([2, 2])\n",
    "mean_2 = np.array([-4, -4])\n",
    "\n",
    "cov_1 = np.array([[2, 0], [0, 2]])\n",
    "cov_2 = np.array([[6, 0], [0, 6]])\n",
    "\n",
    "X1 = (np.random.multivariate_normal(mean_1, cov_1, 128))\n",
    "X2 = (np.random.multivariate_normal(mean_2, cov_2, 128))\n",
    "\n",
    "\n",
    "# Affichage des données\n",
    "\n",
    "plt.plot(X1[:, 0], X1[:,1], \"o\", label = 'Individu', color = \"black\")\n",
    "plt.plot(X2[:, 0], X2[:,1], \"o\", label = 'Individu', color = \"red\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.1.2.1 Test de la Méthode de `K-means`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n",
    "clusters = 5\n",
    "\n",
    "kmeans = KMeans(n_clusters=clusters, n_init=1, init='k-means++')\n",
    "\n",
    "data = np.concatenate((X1, X2), axis=0)\n",
    "kmeans.fit(data)\n",
    "\n",
    "y_kmeans = kmeans.predict(data)\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Affichage d'un diagramme de Voronoi à partir des clusters. Hors TP\n",
    "# if clusters > 2:\n",
    "#     bounds = Voronoi(centers)\n",
    "#     voronoi_plot_2d(bounds)\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=y_kmeans)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "print('Taux d\\'erreur : ' + str(adjusted_rand_score(kmeans.labels_, y_kmeans)))\n",
    "if (adjusted_rand_score(kmeans.labels_, y_kmeans) == 1.0):\n",
    "    print(\"Les clusters sont identiques\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `kmeans.labels_` correspond à la liste des clusters associés à chaque point. Par exemple, si `kmeans.labels_[0] == 1`, cela signifie que le points n°0 appartient au cluster n°1. On note que si `K` le nombre de clusters demandé, alors `0 <= kmeans.labels_[x] < K`.\n",
    "\n",
    "Avec `K = 5`, nous avons le résultat affiché au dessus.\n",
    "\n",
    "D'après la documentation, si `adjusted_rand_score() == 1`, alors les deux groupes de clusters comparés sont identiques (à une permutation près).\n",
    "\n",
    "Le paramètre `n_init` donne le nombre d'itérations de calcul à faire (en partant de points aléatoires différents à chaque fois), avant de choisir le meilleur résultat. Dans notre cas, nous n'avons pas remarqué de résultats significativement différents, cependent le temps de calcul a été évidemment `n_init` fois plus important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1.2.2 Choix du bon nombre de cluster en utilisant les silhouettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def plot_inertia(data_kmeans):\n",
    "    clusters_max = 15\n",
    "    s_coefs = []\n",
    "    s_intertia = []\n",
    "    for i in range(2, clusters_max) :\n",
    "        kmeans = KMeans(n_clusters=i, n_init=1, init='k-means++').fit(data_kmeans)\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        s_avg = silhouette_score(data_kmeans, labels)\n",
    "        print(str(i) + \" clusters, coef de silhouette : \" + str(s_avg) + \", intertie : \" + str(kmeans.inertia_))\n",
    "        s_coefs.append(s_avg)\n",
    "        s_intertia.append(kmeans.inertia_)\n",
    "        # print(\"Sample silhouete value : \" + str(silhouette_samples(data, labels)))\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel(\"nb clusters\")\n",
    "    ax1.set_ylabel(\"coef de silhouette\")\n",
    "    ax1.plot(range(2, clusters_max), s_coefs, color=\"blue\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"inertia\", color=\"red\")\n",
    "    ax2.plot(range(2, clusters_max), s_intertia, color=\"red\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"red\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_inertia(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de clusters idéal est déterminé par un coude sur le graphique de coefficients de silhouette. D'apres le graphique ci-dessus, nous pouvons distinguer un coude pour `K = 6`, le nombre de clusters idéal est donc 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1.3 Clustering Ascendant Hiérarchique (CAH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage as CAH, dendrogram, fcluster\n",
    "\n",
    "clusters = 3\n",
    "\n",
    "def plot_z_complete(method):\n",
    "    plt.gca().set_title(method)\n",
    "    Z_complete = CAH(data, method=method, metric='euclidean')\n",
    "\n",
    "    threshold_value = Z_complete[Z_complete.shape[0] - clusters, 2]\n",
    "\n",
    "    plt.axhline(y=threshold_value, c='grey', lw=1, linestyle='dashed')\n",
    "    threshold_value_before = Z_complete[Z_complete.shape[0] - clusters + 1, 2]\n",
    "    dendrogram(Z_complete,  color_threshold=threshold_value_before)\n",
    "    groupes_cah = fcluster(Z_complete, t=threshold_value, criterion=\"distance\")\n",
    "    print(\"Méthode : \" + method + \"\\t\" + str(adjusted_rand_score(groupes_cah, y_kmeans)))\n",
    "\n",
    "plt.title(\"CAH\")\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_z_complete('ward')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_z_complete('complete')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_z_complete('single')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_z_complete('average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 3\n",
    "\n",
    "kmeans = KMeans(n_clusters=clusters, n_init=1, init='k-means++')\n",
    "kmeans.fit(data)\n",
    "y_kmeans = kmeans.predict(data)\n",
    "\n",
    "def compare_z_complete(method):\n",
    "    Z_complete = CAH(data, method=method, metric='euclidean')\n",
    "    threshold_value = Z_complete[Z_complete.shape[0] - clusters, 2]\n",
    "    groupes_cah = fcluster(Z_complete, t=threshold_value, criterion=\"distance\")\n",
    "    print(\"Méthode : \" + method + \"\\t\" + str(adjusted_rand_score(groupes_cah, y_kmeans)))\n",
    "    \n",
    "compare_z_complete('ward')\n",
    "compare_z_complete('complete')\n",
    "compare_z_complete('single')\n",
    "compare_z_complete('average')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour `K = 2`, n remarque aussi que les clusters formés par les méthodes `ward`, `complete` et `average` sont très proches des résultats obtenus avec `K-Means` (adjusted_rand_score > 0.9). Cependant la méthode `single` donne des résultats très différents (adjusted_rand_score < 0.15)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de la méthode K-Means pour la réduction de couleur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "plt.rcParams['figure.figsize'] = (15, 15)\n",
    "\n",
    "# Affiche l'image après réduction à i couleurs (via clusters) \n",
    "def image_kmeans(i):\n",
    "    plt.title(str(i) + \" clusters\")\n",
    "    \n",
    "    raw_img = mpimg.imread('visage.bmp')\n",
    "    img = np.float32(raw_img)\n",
    "    a = np.reshape(img, (img.shape[0]*img.shape[1],3))\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=i,n_init=1,init='k-means++')\n",
    "    kmeans.fit(a)\n",
    "\n",
    "    for i in range(a.shape[0]):\n",
    "        a[i] = kmeans.cluster_centers_[kmeans.labels_[i],:]\n",
    "    \n",
    "    res_img = np.reshape(a, (img.shape[0], img.shape[1], 3))\n",
    "    plt.imshow(res_img/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [5, 15, 30, 50, 75, 100, 150, 200]\n",
    "\n",
    "\n",
    "for index, k in enumerate(K):\n",
    "    plt.subplot(3, 3, index + 1)\n",
    "    image_kmeans(k)\n",
    "    \n",
    "plt.subplot(3, 3, 9)\n",
    "plt.imshow(mpimg.imread('visage.bmp'))\n",
    "plt.title(\"image initiale\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que le bleu est minime par rapport au rouge et au vert, il a donc tendance à rentrer dans les clusters de ceux-ci, ce qui explique la couleur plus orangé des yeux sur l'image traité avec le kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering de données de températures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clusters = 7\n",
    "\n",
    "data_temperature = pd.read_csv(\"temperatures.csv\",sep=\";\",decimal=\".\",header=0,index_col = 0)\n",
    "# n = len(data_temperature)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title(\"Janvier -> Décembre\")\n",
    "data = data_temperature.drop(columns=['Region', 'Moyenne', 'Amplitude', 'Latitude', 'Longitude'])\n",
    "Z_complete = CAH(data, method='average', metric='euclidean')\n",
    "\n",
    "threshold_value = Z_complete[Z_complete.shape[0] - clusters, 2]\n",
    "plt.axhline(y=threshold_value, c='grey', lw=1, linestyle='dashed')\n",
    "city_name = list(data.index)\n",
    "a = dendrogram(Z_complete, labels=city_name, color_threshold=threshold_value)\n",
    "plt.subplot(2, 2, 2)\n",
    "coord = data_temperature.loc[:, ['Latitude', 'Longitude']].values\n",
    "#Cette ligne permet d’extraire les coordonnees\n",
    "groupes_cah_1 = fcluster(Z_complete, t=threshold_value, criterion=\"distance\")\n",
    "plt.scatter(coord[:, 1], coord[:, 0], cmap='viridis', c=groupes_cah_1, marker=\"o\", s=200)\n",
    "#On place les points\n",
    "for i, txt in enumerate(city_name):\n",
    "    plt.annotate(txt, (coord[i, 1], coord[i, 0])) #On place le nom des villes\n",
    "    \n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title(\"Juillet -> Décembre, Moy, Amplitude, Latitude et Longitude\")\n",
    "data = data_temperature.drop(columns=['Janvier', 'Fevrier', 'Mars', 'Avril', 'Mai', 'Juin', 'Region'])\n",
    "Z_complete = CAH(data, method='average', metric='euclidean')\n",
    "threshold_value = Z_complete[Z_complete.shape[0] - clusters, 2]\n",
    "plt.axhline(y=threshold_value, c='grey', lw=1, linestyle='dashed')\n",
    "city_name = list(data.index)\n",
    "a = dendrogram(Z_complete, labels=city_name, color_threshold=threshold_value)\n",
    "plt.subplot(2, 2, 4)\n",
    "coord = data_temperature.loc[:, ['Latitude', 'Longitude']].values\n",
    "#Cette ligne permet d’extraire les coordonnees\n",
    "groupes_cah_2 = fcluster(Z_complete, t=threshold_value, criterion=\"distance\")\n",
    "plt.scatter(coord[:, 1], coord[:, 0], cmap='viridis', c=groupes_cah_2, marker=\"o\", s=200)\n",
    "#On place les points\n",
    "for i, txt in enumerate(city_name):\n",
    "    plt.annotate(txt, (coord[i, 1], coord[i, 0])) #On place le nom des villes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clusters_cities = 7\n",
    "\n",
    "kmeans = KMeans(n_clusters=nb_clusters_cities, n_init=10, init='k-means++').fit(coord)\n",
    "y_kmeans = kmeans.predict(coord)\n",
    "\n",
    "plt.scatter(coord[:, 1], coord[:, 0], c=y_kmeans, marker=\"o\", s=200)\n",
    "\n",
    "for i, txt in enumerate(city_name):\n",
    "    plt.annotate(txt, (coord[i, 1], coord[i, 0])) #On place le nom des villes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inertia(coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la règle du coude, on peut estimer la valeur optimale de `K = 7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Groupe 1 : \" + str(adjusted_rand_score(groupes_cah_1, y_kmeans)))\n",
    "print(\"Groupe 2 : \" + str(adjusted_rand_score(groupes_cah_2, y_kmeans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque les clusters formés par le groupe de donnés 1 (Janvier -> Décembre) sont assez différents de ceux formés par kmeans, aussi, les clusters formés par le groupe de données 2 (Juillet -> Décembre, Moyenne, Amplitude, Latitude et Longitude) est un peu plus proche du résultat de kmeans que le groupe 1 même s'il reste peu similaire."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
